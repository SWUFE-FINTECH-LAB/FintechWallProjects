# Wind 市场信息墙 – 测试标准与流程

## 1. 目的与范围
建立一个可重复的测试框架，确保 Wind 版和开源版的市场信息墙都能提供准确、及时、可靠的市场洞察。本计划涵盖后端服务（Python）、前端轮播（HTML/CSS/JavaScript）、数据管道和信息亭部署工作流。所有测试活动必须确保版本之间的对等性，重点关注数据完整性和弹性。

## 2. 测试目标
- 验证产品需求文档（PRD）中所有场景和轮播行为的功能性需求。
- 确保两个数据提供商堆栈的数据正确性、新鲜度和回退逻辑。
- 确认在 4K 分辨率下的前端渲染、可访问性和信息亭可用性。
- 证明系统在实时数据流下持续 72 小时运行的稳定性。
- 提供可审计的证据，证明发布候选版本在部署前符合验收标准。

## 3. 测试环境
| 环境 | 目的 | 数据源 | 备注 |
| --- | --- | --- | --- |
| **开发环境 (DEV)** | 快速迭代，单元/集成测试 | 模拟提供商、录制的数据 | 通过 Docker Compose 自动配置 |
| **质量保证环境 (QA - Wind)** | 使用 Wind 数据进行端到端验证 | 实时 WindPy（沙盒凭证） | 限制在内部网络 |
| **质量保证环境 (QA - Open)** | 使用开放/免费 API 进行端到端验证 | 有速率限制的公共 API | 使用节流和缓存以避免被封禁 |
| **试点环境 (Pilot)** | 在信息亭硬件上进行演练 | 与生产环境相同 | 72 小时老化测试 |

## 4. 测试类别
### 4.1 单元测试
- **后端**: 使用 Pytest 对提供商适配器、数据规范化器、调度器、WebSocket 管理器进行覆盖率测试。
- **前端**: 使用 Jest/Vitest 对实用工具模块（数据格式化、调度）进行测试。
- **成功标准**: 关键模块（提供商、调度器、轮播计时）的行覆盖率 ≥ 85%。

### 4.2 提供商合约测试
- 使用模拟响应和实时健全性检查，验证 Wind 和开放提供商的模式合规性。
- 确保每个提供商返回强制字段（`code`、`label`、`value`、`timestamp`、`source`）。
- 在持续集成（CI）中使用基于 JSON Schema 或 Pydantic 的合约测试。

### 4.3 数据质量与新鲜度
- 自动化验证（使用 Great Expectations 或自定义检查）：
  - 每个资产类别的时间戳新鲜度与服务等级协议（SLA）的对比。
  - 数值健全性（例如，价格 > 0，日内百分比变化在 ±50% 内，除非有特殊标记）。
  - 跨字段一致性（例如，稳定币主导地位总和 ≤ 100%）。
- 当连续失败次数超过阈值时触发警报。

### 4.4 集成测试
- 使用 Docker 化的 Redis 端到端地执行数据注入工作流。
- 使用录制的数据（VCR.py/Betamax）模拟 API 响应，以实现确定性的 CI 运行。
- 验证两个版本中的回退链（主提供商失败 → 辅助提供商成功 → 缓存更新）。

### 4.5 系统与功能测试
- 基于场景的测试，确保轮播、会话权重、过时指示器和风险警报按照 PRD 的规定运行。
- 使用 Playwright 编写信息亭视口（3840×2160）脚本，并断言每个场景的 DOM 状态。

### 4.6 UI/UX 与可访问性测试
- 使用 Playwright 截图对比或 BackstopJS 对关键场景进行视觉回归测试。
- 通过 Axe-core 进行可访问性审计（对比度、ARIA 角色、键盘导航，即使信息亭是被动式的）。
- 验证字体大小和调色板符合设计规范。

### 4.7 性能与负载测试
- 使用 Locust/k6 进行后端吞吐量测试，模拟并发的信息亭连接（基准 5 个客户端，预留 20 个）。
- 在负载下测量 WebSocket 延迟（<1 秒）和 REST 响应时间（<500 毫秒）。
- 使用 Lighthouse 测量前端渲染性能（模拟 4K 显示，确保没有超过 200 毫秒的长任务）。

### 4.8 弹性与故障转移测试
- 混沌场景：Wind 服务中断、开放 API 速率限制、Redis 重启、信息亭网络中断。
- 验证回退逻辑、过时标记行为和自动恢复。
- 进行 72 小时浸泡测试，并使用自动化监控器捕捉内存泄漏或计时器漂移。

### 4.9 安全与合规性检查
- 验证 HTTPS 强制执行、CORS 限制以及新闻标题的净化（剥离 HTML/JS）。
- 静态分析（Bandit、Ruff）和依赖项漏洞扫描（pip-audit、npm audit for static assets）。

### 4.10 回归测试
- 每次合并到主分支时触发自动化套件；包括单元、集成、合约和关键的 Playwright 脚本。
- 发布候选版本必须在两个版本的 QA 环境中运行完整的回归测试。

### 4.11 用户验收测试 (UAT)
- 使用源自 PRD 验收标准清单，与试点利益相关者进行结构化演练。
- 记录签收并捕获反馈，用于待办事项的梳理。

## 5. 测试数据管理
- **Wind 版**: 使用专用的 Wind 沙盒账户；记录 API 使用情况以确保配额得到遵守。
- **开源版**: 缓存示例负载以减少实时调用；在数据中屏蔽 API 密钥。
- 在版本控制中存储确定性的数据，用于可重复的单元/集成测试。
- 对于敏感数据（如有），应用匿名化或使用合成替代品。

## 6. 自动化与工具
| 领域 | 工具 | 备注 |
| --- | --- | --- |
| 单元/集成 | Pytest, pytest-asyncio, VCR.py | 在 CI 中运行并生成覆盖率报告 |
| 模式验证 | Pydantic 模型, jsonschema | 强制执行规范的负载结构 |
| 前端测试 | Playwright, Vitest | 4K 分辨率下的无头和有头模式 |
| 视觉回归 | Playwright 跟踪或 BackstopJS | 每个版本存储基准图像 |
| 可访问性 | Axe-core CLI, Lighthouse | 在 CI 中每晚自动执行 |
| 性能 | k6 或 Locust | 编写 REST 和 WebSocket 的场景脚本 |
| 监控 | Prometheus + Grafana (QA/Pilot) | 在浸泡测试期间捕获指标 |
| 静态分析 | Ruff, Bandit, ESLint | 在提交前和 CI 中强制执行 |

## 7. 测试流程
1. **规划**
   - 定义每个里程碑的测试计划，将需求映射到测试用例，识别风险。
   - 更新将 PRD 功能与测试覆盖率联系起来的可追溯性矩阵。
2. **准备**
   - 配置环境（DEV 使用 Docker Compose，QA 使用沙盒凭证）。
   - 生成或刷新数据；配置 Playwright 基准。
3. **执行**
   - 在每个拉取请求上通过 CI 运行自动化套件。
   - 在主要功能开发后，在 QA 环境中进行手动探索性测试（重点关注布局和信息亭感觉）。
4. **缺陷管理**
   - 在跟踪器中记录缺陷（严重性、版本、场景、测试参考）。
   - 每日进行分类；关键问题在解决前会阻止发布。
5. **报告**
   - 发布 CI 仪表板（覆盖率、通过/失败）和每周 QA 摘要。
   - 对于试点运行，捕获正常运行时间、数据新鲜度指标和用户反馈。
6. **发布准备**
   - 验证进入标准：所有优先测试用例已执行，没有未解决的 Sev1/Sev2 问题，两个版本的回归测试均已通过。
   - 退出标准：UAT 签收，浸泡测试成功，文档已更新。

## 8. 角色与职责
| 角色 | 职责 |
| --- | --- |
| QA 负责人 | 维护测试策略，协调日程，批准发布准备 |
| 后端工程师 | 实现提供商/单元测试，修复数据注入缺陷 |
| 前端工程师 | 维护 Playwright 套件，视觉基准 |
| DevOps | 管理 QA 环境，监控浸泡测试 |
| 产品负责人 | 审查 UAT 结果，批准验收标准 |

## 9. 测试进度安排
- **阶段 1 (基础)**: 场景 A 提供商的单元和合约测试；CI 管道基准。
- **阶段 2 (数据覆盖)**: 场景 B-E 的集成测试；初始 Playwright 脚本。
- **阶段 3 (前端 MVP)**: UI 回归和可访问性冒烟测试。
- **阶段 4 (视觉增强)**: 更新视觉基准；在新图表上运行性能基准测试。
- **阶段 5 (调度/主题)**: 会话权重和暗黑模式的场景测试。
- **阶段 6 (弹性)**: 在 QA 环境中执行混沌和 72 小时浸泡测试。
- **阶段 7 (打包)**: 最终回归、UAT 和试点验证。

## 10. 质量指标
- 自动化测试通过率（构建级别）。
- 代码覆盖率百分比（后端、前端）。
- 数据新鲜度合规性（按资产类别划分的 SLA 内百分比）。
- 缺陷密度（每个功能的缺陷数）和平均解决时间。
- 试点老化测试期间的正常运行时间。

## 11. 交付物
- 主测试计划文档（动态更新，存储在 `docs/` 中）。
- 与功能相关联的测试用例库或电子表格。
- 版本控制中的自动化测试套件，带有 CI 徽章。
- 每个里程碑的 QA 运行报告，包括缺陷和指标。
- 包含签收记录的最终发布测试摘要。

## 12. 审查与持续改进
- 每次发布后进行回顾，以改进测试用例、工具和自动化差距。
- 每季度或在重大市场事件改变视觉预期时更新数据和基准。
- 跟踪不稳定的测试；在一个 sprint 内修复。